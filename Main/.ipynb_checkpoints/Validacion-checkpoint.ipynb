{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c562c3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@52256b8c\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c87fb096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.collection.mutable.WrappedArray\n",
       "import org.apache.spark.sql.{Column, Row}\n",
       "import org.apache.spark.sql.functions.{col, when}\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.WrappedArray\n",
    "import org.apache.spark.sql.{Column, Row}\n",
    "import org.apache.spark.sql.functions.{col,when}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "21a8698b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "validations: String = /data/validation.json\n",
       "data: org.apache.spark.sql.DataFrame = [id: string, age: string]\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val validations = \"/data/validation.json\"\n",
    "val data = spark.read.option(\"header\", true).csv(\"/data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7c71d49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      "\n",
      "+---+----+\n",
      "| id| age|\n",
      "+---+----+\n",
      "|  1|  10|\n",
      "|  2|  18|\n",
      "|  3| 100|\n",
      "|  4| 120|\n",
      "|  5|null|\n",
      "|  6|text|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()\n",
    "data.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0c6dff1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "validateRange: (name: org.apache.spark.sql.Column, max: Integer, min: Integer)org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validateRange(name: Column, max: Integer, min: Integer=null): Column = {\n",
    "    println(s\"Validator: validateRange, column: ${name}, params: ${min}, ${max}\")\n",
    "    if(null == max) {\n",
    "        name >=min    \n",
    "    }else{\n",
    "        name >= min && name < max\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dab4daef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isNaN: (name: org.apache.spark.sql.Column)org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def isNaN(name: Column): Column = {\n",
    "    println(s\"Validator: isNaN, column: ${name}\")\n",
    "    name.isNaN\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aa15486e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selectValidator: (validator: org.apache.spark.sql.Row)org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def selectValidator(validator: Row): Column = {\n",
    "    print(validator)\n",
    "    val column = col(validator.getString(0))\n",
    "    val func = validator.getString(2)\n",
    "    println(s\"column:$column\")\n",
    "    println(s\"func:$func\")\n",
    "    func match {\n",
    "        case \"isNaN\" => isNaN(column)\n",
    "        case \"range\" => validateRange(column, validator.getLong(3).toInt, validator.getLong(4).toInt) \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c350f4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "transformValidations: (dfValidations: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame   \n",
    "def transformValidations(dfValidations: DataFrame )={\n",
    "       val dfExplode=  dfValidations.withColumn(\"validators\",explode(col(\"validators\")))\n",
    "      .withColumn(\"level\",$\"validators.level\")\n",
    "      //.withColumn(\"validator\",$\"validators.validator\")\n",
    "      .withColumn(\"func\",$\"validators.validator.func\")\n",
    "      .withColumn(\"max\",$\"validators.validator.max\")\n",
    "      .withColumn(\"min\",$\"validators.validator.min\")\n",
    "    val columnasQuery = Seq(\"column\",\"level\",\"func\",\"max\",\"min\")\n",
    "    val returndf = dfExplode.select(columnasQuery.head,columnasQuery.tail:_*)\n",
    "       returndf\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8a03172f",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "50: error: too many arguments (2) for method selectValidator: (validator: org.apache.spark.sql.Row)org.apache.spark.sql.Column",
     "output_type": "error",
     "traceback": [
      "<console>:50: error: too many arguments (2) for method selectValidator: (validator: org.apache.spark.sql.Row)org.apache.spark.sql.Column",
      "               cols :+ selectValidator(col(column), validator)",
      "                                                    ^",
      ""
     ]
    }
   ],
   "source": [
    "def crearWiths(row: Row): List[Column] = {\n",
    "    val column = row.getAs[String](\"column\")\n",
    "    \n",
    "    val r = row.getAs[WrappedArray[Row]](\"validators\")\n",
    "    println(s\"${column} -> ${r}\")\n",
    "    val cols = List[Column]()\n",
    "    for(ers <- r) {\n",
    "        val lvl = ers.getAs[String](\"level\")\n",
    "        val validator = ers.getAs[Row](\"validator\")\n",
    "        cols :+ selectValidator(col(column), validator)\n",
    "    }\n",
    "    cols\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "282cbaaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crearWithsOr: (row: org.apache.spark.sql.Row)org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def crearWithsOr (row: Row) = {\n",
    "    println(row)\n",
    "    lit(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f632ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|column|          validators|\n",
      "+------+--------------------+\n",
      "|   age|[{error, {isNaN, ...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option(\"multiline\",\"true\").json(validations).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "99987402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getErrors: ()org.apache.spark.sql.Column\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getErrors() = {\n",
    "    val df = spark.read.option(\"multiline\",\"true\").json(validations)\n",
    "    df.show()\n",
    "    val newValidations = transformValidations(df)\n",
    "    newValidations.show()\n",
    "    newValidations.collect().map(selectValidator(_)).reduce(_ or _)\n",
    "    //.show\n",
    "    //df.\n",
    "    //val creates = df.collect().map(crearWiths(_))\n",
    "    //creates\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "442db875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|column|          validators|\n",
      "+------+--------------------+\n",
      "|   age|[{error, {isNaN, ...|\n",
      "+------+--------------------+\n",
      "\n",
      "+------+-----+-----+----+----+\n",
      "|column|level| func| max| min|\n",
      "+------+-----+-----+----+----+\n",
      "|   age|error|isNaN|null|null|\n",
      "|   age| warn|range| 120|  18|\n",
      "+------+-----+-----+----+----+\n",
      "\n",
      "[age,error,isNaN,null,null]column:age\n",
      "func:isNaN\n",
      "Validator: isNaN, column: age\n",
      "[age,warn,range,120,18]column:age\n",
      "func:range\n",
      "Validator: validateRange, column: age, params: 18, 120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res21: org.apache.spark.sql.Column = (isnan(age) OR ((age >= 18) AND (age < 120)))\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getErrors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "93060b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|column|          validators|\n",
      "+------+--------------------+\n",
      "|   age|[{error, {isNaN, ...|\n",
      "+------+--------------------+\n",
      "\n",
      "+------+-----+-----+----+----+\n",
      "|column|level| func| max| min|\n",
      "+------+-----+-----+----+----+\n",
      "|   age|error|isNaN|null|null|\n",
      "|   age| warn|range| 120|  18|\n",
      "+------+-----+-----+----+----+\n",
      "\n",
      "[age,error,isNaN,null,null]column:age\n",
      "func:isNaN\n",
      "Validator: isNaN, column: age\n",
      "[age,warn,range,120,18]column:age\n",
      "func:range\n",
      "Validator: validateRange, column: age, params: 18, 120\n",
      "+---+----+-----+\n",
      "| id| age|error|\n",
      "+---+----+-----+\n",
      "|  1|  10|false|\n",
      "|  2|  18| true|\n",
      "|  3| 100| true|\n",
      "|  4| 120|false|\n",
      "|  5|null| null|\n",
      "|  6|text| null|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.withColumn(\"error\", getErrors).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b51edf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
